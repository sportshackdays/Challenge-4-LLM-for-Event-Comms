{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import json \n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token $HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and read the JSON file\n",
    "with open('email_data_set.json', 'r') as file:\n",
    "    email_data = json.load(file)\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open('scraped_data.json', 'r') as file:\n",
    "    scraped_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_thread(email_id):\n",
    "    input_text = \"\"\n",
    "    labels = []\n",
    "    for thread in email_data[email_id]['conversation']:\n",
    "      labels.append(thread['category'])\n",
    "      # add the sender\n",
    "      if thread['direction'] == 'incoming':\n",
    "          input_text += \"Customer Email: \" + thread['body'] + \"\\n\"\n",
    "      elif thread['direction'] == 'outgoing':\n",
    "          input_text += \"Customer Service Email: \" + thread['body'] + \"\\n\"\n",
    "      elif thread['direction'] == 'forwarded':\n",
    "          input_text += \"Third Party Email: \" + thread['body'] + \"\\n\"      \n",
    "\n",
    "    return input_text, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean markdown content by removing unnecessary whitespace and characters\n",
    "def clean_markdown(text):\n",
    "    # Check if the text is a valid string\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove markdown links/images\n",
    "    text = re.sub(r'!\\[.*?\\]\\(.*?\\)', '', text)  # Remove images\n",
    "    text = re.sub(r'\\[.*?\\]\\(.*?\\)', '', text)  # Remove links\n",
    "    # Normalize whitespace (remove extra newlines, tabs, etc.)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "# Process the scraped data\n",
    "for entry in scraped_data:\n",
    "    markdown_content = entry.get(\"markdown\", \"\")\n",
    "    \n",
    "    # Clean the markdown content if it's a string, otherwise set it to empty\n",
    "    if isinstance(markdown_content, str):\n",
    "        entry[\"cleaned_markdown\"] = clean_markdown(markdown_content)\n",
    "    else:\n",
    "        entry[\"cleaned_markdown\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract information from the contact form and parse the query\n",
    "def extract_contact_form_info(query):\n",
    "    # Regex patterns to extract various fields based on the structure of the input\n",
    "    form_data = {\n",
    "        \"event\": re.search(r'Veranstaltung:\\s*([^\\s]+)', query),  # Match the event after \"Veranstaltung:\"\n",
    "        \"first_name\": re.search(r'Vorname:\\s*([^\\s]+)', query),  # Match the first name after \"Vorname:\"\n",
    "        \"last_name\": re.search(r'Nachname:\\s*([^\\s]+)', query),  # Match the last name after \"Nachname:\"\n",
    "        \"birth_date\": re.search(r'Geburtsdatum:\\s*([^\\s]+)', query),  # Match the birth date after \"Geburtsdatum:\"\n",
    "        \"address\": re.search(r'Adresse:\\s*(.+?)\\s*E-Mail:', query),  # Match the address until \"E-Mail:\"\n",
    "        \"email\": re.search(r'E-Mail:\\s*([^\\s]+)', query),  # Match the email after \"E-Mail:\"\n",
    "        \"phone\": re.search(r'Telefon:\\s*([^\\s]+)', query),  # Match the phone number after \"Telefon:\"\n",
    "        \"message\": re.search(r'Mitteilung:\\s*(.*)', query, re.DOTALL)  # Capture everything after \"Mitteilung\"\n",
    "    }\n",
    "    \n",
    "    # Extract and clean values\n",
    "    extracted_info = {key: match.group(1).strip() if match else None for key, match in form_data.items()}\n",
    "    return extracted_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created with the embeddings.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained Sentence Transformer model for embedding generation\n",
    "sentence_model = SentenceTransformer('all-mpnet-base-v2')  # You can choose other models as well\n",
    "\n",
    "# Generate embeddings for the cleaned markdown content\n",
    "for entry in scraped_data:\n",
    "    entry['embedding'] = sentence_model.encode(entry['cleaned_markdown'])\n",
    "\n",
    "# Convert embeddings to a numpy array\n",
    "embeddings = np.array([entry['embedding'] for entry in scraped_data])\n",
    "\n",
    "# Ensure embeddings are created\n",
    "assert len(embeddings) > 0, \"No embeddings found. Ensure embeddings are properly generated.\"\n",
    "\n",
    "# Get the embedding dimension from the model (i.e., the size of each embedding vector)\n",
    "dimension = embeddings.shape[1]  # Typically something like 768 for transformers\n",
    "\n",
    "# Create FAISS index for L2 (Euclidean) similarity\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add the embeddings to the FAISS index\n",
    "index.add(embeddings)\n",
    "\n",
    "print(\"FAISS index created with the embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "generator = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# sent model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = generator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "API_URL = \"https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def send_to_huggingface_api(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "# Define a retry mechanism to wait for the model to load\n",
    "def send_with_retry(payload, max_retries=10, wait_time=30):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        response = send_to_huggingface_api(payload)\n",
    "        if 'error' in response and 'loading' in response['error']:\n",
    "            print(f\"Model is loading, retrying in {wait_time} seconds... ({response['estimated_time']} seconds estimated)\")\n",
    "            time.sleep(wait_time)  # Wait for the estimated time or a fixed amount before retrying\n",
    "            retries += 1\n",
    "        else:\n",
    "            return response\n",
    "    return {\"error\": \"Model failed to load after multiple retries.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The RAG pipeline function\n",
    "def rag_pipeline(query, sentence_model, index, generator, tokenizer, scraped_data):\n",
    "    # Step 1: Parse the query and extract contact form information\n",
    "    contact_info = extract_contact_form_info(query)\n",
    "    \n",
    "    # Extract relevant parts from the contact form\n",
    "    cleaned_query = contact_info[\"message\"]\n",
    "    \n",
    "    # Step 2: Encode the cleaned query and retrieve relevant documents\n",
    "    query_embedding = sentence_model.encode(cleaned_query)\n",
    "    \n",
    "    # Retrieve top-k relevant documents (using FAISS or other retrieval methods)\n",
    "    k = 3\n",
    "    distances, indices = index.search(np.array([query_embedding]), k)\n",
    "    retrieved_docs = [scraped_data[i] for i in indices[0]]\n",
    "    \n",
    "    # Step 3: Combine retrieved documents into a single context\n",
    "    #context = \" \".join([doc['cleaned_markdown'] for doc in retrieved_docs])\n",
    "    context = \" \".join([doc['cleaned_markdown'][:1500] for doc in retrieved_docs])\n",
    "\n",
    "    # Step 4: Create a prompt with an email structure\n",
    "    prompt = f\"\"\"\n",
    "    The user has the following query: \"{cleaned_query}\"\n",
    "    Use the context provided below to craft a professional response as an email.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Email format:\n",
    "    - Start with a greeting (e.g., \"Dear [name],\")\n",
    "    - Include a clear response addressing the user's query.\n",
    "    - End with a professional sign-off (e.g., \"Best regards, \\nYour DataSport Team\" or \"Sincerely, \\nYour DataSport Team\").\n",
    "    - No more text after this.\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    # pipe = pipeline(task=\"text-generation\", model=generator, tokenizer=tokenizer, max_length=len(prompt)+100, device = device)\n",
    "    # result = pipe(f\"[INST] {prompt} [/INST]\")\n",
    "    # generated_response = result[0]['generated_text'][len(prompt):]\n",
    "\n",
    "    # Send the prompt to Hugging Face API with retry logic\n",
    "    payload = {\n",
    "       \"inputs\": prompt,\n",
    "       \"parameters\": {\n",
    "           \"max_new_tokens\": 500,  # Specify the number of new tokens to generate\n",
    "           \"stop\": [\"Best regards, \\nYour DataSport Team\"],\n",
    "           \"temperature\": 0.9,\n",
    "       }\n",
    "    }\n",
    "    api_response = send_with_retry(payload)\n",
    "    \n",
    "    # Extract the generated text from the API response\n",
    "    if 'error' in api_response:\n",
    "       return f\"Error: {api_response['error']}\"\n",
    "    \n",
    "    generated_response = api_response[0]['generated_text'][len(prompt):]\n",
    "    \n",
    "    return generated_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dear <FRIST_NAME_2>,\n",
      "     We apologize for the inconvenience you're experiencing with your DataSport account. We've found that the email address linked to your account is no longer valid, which may have caused you to miss important updates, such as confirmation of your entry and information about your Swiss runners ticket. We're on it! Our team is working to resolve this issue as soon as possible. We will contact you shortly to confirm the updated information. If you have any further questions or concerns, feel free to reach out. We appreciate your patience and cooperation in this matter.\n",
      "     Best regards,\n",
      "     Your DataSport Team. IP: <IP_ADDRESS_1> \"\"\n",
      "\n",
      "\n",
      "    Note: The user's first name and IP address will be filled in automatically. Also, the email address should be removed from the response as it's no longer valid.\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the RAG pipeline\n",
    "query = \"Mitteilung: Hello there is a problem with my data sport account. I believe you still have an old email address linked to it (<EMAIL_1>). This email address is no longer valid. I have not received confirmation of my number\\/ entry for Saturday's race (Davos X trails) nor have I received information\\/ code for the Swiss runners ticket. Please can you contact me urgently to resolve this. Thank you. Best regards, <FRIST_NAME_2> IP: <IP_ADDRESS_1>\"\n",
    "            \n",
    "# Generate the response\n",
    "generated_answer = rag_pipeline(query, sentence_model, index, generator, tokenizer, scraped_data)\n",
    "print(generated_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/myenv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from rag_pipeline import get_email\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer:\n",
      "  Dear [FIRST_NAME_1],\n",
      "\n",
      "     Thank you for your interest in our services. We understand that you are looking for the perfect application tips and templates to help you with your job search. As a valued user, we are happy to provide you with some recommendations based on your browsing history on JobScout24.\n",
      "\n",
      "     Please find attached the link to view our recommendations in a webpage format: [URL_1]. Additionally, you may be interested in exploring the jobs we have curated specifically for you at [URL_2].\n",
      "\n",
      "     If you need any assistance with creating a standout application, our various tips and templates for German and French applications are available for you to review.\n",
      "\n",
      "     This email was sent to you based on your browsing activity on JobScout24 and your communication preferences. If you would like to change or stop receiving these emails, please visit your application profile to opt out.\n",
      "\n",
      "     We hope you find these recommendations helpful in your job search. If you have any further questions or need additional guidance, please do not hesitate to reach out to us.\n",
      "\n",
      "     Best regards,\n",
      "     Your DataSport Team\n",
      "    Similar emails could be sent with slight variations depending on the context and your specific needs. This example assumes that the response will be sent in a professional manner.\n"
     ]
    }
   ],
   "source": [
    "query = \"<FIRST_NAME_1>, here are our recommendations based on what you viewed - \\t?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? Not rendering correctly or images not showing? View in a webpage <URL_1> Jobs you might be interested in <URL_2> And if you need a little help putting together the perfect application, check out our various tips and templates ? (German French only) This email was intended for <NAME_1> <EMAIL_1>. You are receiving our recommendations by email based on what you have been browsing on JobScout24 and on your communication preferences. You can change or stop receiving these emails by opting out in your application profile. Unsubscribe from this e-mail | Help | Privacy Policy Copyright \\u00a9 2023 | JobCloud AG, JobScout24.ch, <ADDRESS_1>\"\n",
    "\n",
    "get_email(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
